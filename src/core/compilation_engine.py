"""CompilationEngine for generating executable PyTorch code from validated graphs."""

import logging
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

from jinja2 import Template

from .block_registry import BlockRegistry
from .constraint_solver import Configuration
from .graph_loader import ArchitectureGraph, GraphEdge, GraphNode

logger = logging.getLogger(__name__)


class CompilationError(Exception):
    """Raised when code generation fails."""

    pass


# PyTorch module template using Jinja2
MODULE_TEMPLATE = """# Generated by NeuroScript v2 CompilationEngine
# Do not edit this file manually

import torch
import torch.nn as nn
from typing import Any, Dict, Optional, Tuple

{{ imports }}


class {{ class_name }}(nn.Module):
    \"\"\"
    {{ architecture_name }} - Generated neural architecture.

    Configuration:
{{ config_doc }}

    Graph topology:
{{ topology_doc }}
    \"\"\"

    def __init__(self):
        super().__init__()

        # Initialize blocks
{{ block_init }}

    def forward(self, {{ forward_args }}) -> {{ forward_return }}:
        \"\"\"
        Forward pass through the architecture.

        Args:
{{ forward_args_doc }}

        Returns:
{{ forward_return_doc }}
        \"\"\"
{{ shape_assertions }}

        # Execute computation graph
{{ forward_body }}

        return {{ return_statement }}
"""


class CompilationEngine:
    """
    Generates executable PyTorch code from validated architecture graphs.

    Implements Req 8.1, 8.2, 8.3, 8.4, 8.5
    """

    def __init__(
        self,
        graph: ArchitectureGraph,
        config: Configuration,
        registry: BlockRegistry
    ):
        """
        Initialize CompilationEngine.

        Args:
            graph: Validated architecture graph
            config: Concrete dimension configuration
            registry: Block registry for accessing block metadata
        """
        self.graph = graph
        self.config = config
        self.registry = registry
        self._node_map: Dict[str, GraphNode] = {node.id: node for node in graph.nodes}
        self._execution_order: Optional[List[str]] = None

    def compile(self, output_path: Path, class_name: str = "GeneratedModel") -> None:
        """
        Generate PyTorch module file.

        Implements Req 8.1, 8.2

        Args:
            output_path: Path to write generated Python module
            class_name: Name for generated nn.Module class

        Raises:
            CompilationError: If code generation fails
        """
        logger.info(f"Compiling architecture to {output_path}")

        try:
            # Compute execution order
            self._execution_order = self._compute_execution_order()

            # Generate all code components
            imports = self._generate_imports()
            config_doc = self._generate_config_doc()
            topology_doc = self._generate_topology_doc()
            block_init = self._generate_block_init()
            forward_args, forward_args_doc = self._generate_forward_signature()
            forward_return, forward_return_doc = self._generate_return_signature()
            shape_assertions = self._generate_shape_assertions()
            forward_body = self._generate_forward_body()
            return_statement = self._generate_return_statement()

            # Render template
            template = Template(MODULE_TEMPLATE)
            code = template.render(
                class_name=class_name,
                architecture_name=self.graph.metadata.get('name', 'UnnamedArchitecture'),
                imports=imports,
                config_doc=config_doc,
                topology_doc=topology_doc,
                block_init=block_init,
                forward_args=forward_args,
                forward_args_doc=forward_args_doc,
                forward_return=forward_return,
                forward_return_doc=forward_return_doc,
                shape_assertions=shape_assertions,
                forward_body=forward_body,
                return_statement=return_statement,
            )

            # Format code
            code = self._format_code(code)

            # Write to file
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            output_path.write_text(code)

            logger.info(f"Successfully compiled to {output_path}")

        except Exception as e:
            raise CompilationError(f"Failed to compile architecture: {e}") from e

    def _compute_execution_order(self) -> List[str]:
        """
        Compute topological execution order for nodes.

        Returns:
            List of node IDs in execution order

        Raises:
            CompilationError: If graph has cycles (should not happen after validation)
        """
        # Build adjacency list and in-degree count
        adj: Dict[str, List[str]] = {node.id: [] for node in self.graph.nodes}
        in_degree: Dict[str, int] = {node.id: 0 for node in self.graph.nodes}

        for edge in self.graph.edges:
            if edge.source in adj and edge.target in adj:
                adj[edge.source].append(edge.target)
                in_degree[edge.target] += 1

        # Kahn's algorithm for topological sort
        queue = [node_id for node_id, degree in in_degree.items() if degree == 0]
        order = []

        while queue:
            # Sort for deterministic ordering
            queue.sort()
            node_id = queue.pop(0)
            order.append(node_id)

            for neighbor in adj[node_id]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)

        if len(order) != len(self.graph.nodes):
            raise CompilationError("Graph contains cycles (validation should have caught this)")

        return order

    def _generate_imports(self) -> str:
        """
        Generate import statements for block implementations.

        Implements Req 8.2

        Returns:
            Import statements as string
        """
        imports = set()

        for node in self.graph.nodes:
            block_path = self.registry.get_block_path(node.block_id)
            # Convert block path to Python import
            # Example: blocks/linear -> from blocks.linear.module import Linear
            block_name = node.block_id

            # Get the block capability to find the actual class name
            block_cap = self.registry.get_block(node.block_id)
            class_name = block_cap.name

            import_stmt = f"from blocks.{block_name}.module import {class_name}"
            imports.add(import_stmt)

        return "\n".join(sorted(imports))

    def _generate_config_doc(self) -> str:
        """Generate configuration documentation string."""
        lines = []
        for dim, value in sorted(self.config.bindings.items()):
            lines.append(f"        {dim} = {value}")

        if self.config.estimated_params > 0:
            lines.append(f"        Total parameters: {self.config.estimated_params:,}")
        if self.config.estimated_memory_gb > 0:
            lines.append(f"        Estimated memory: {self.config.estimated_memory_gb:.2f} GB")

        return "\n".join(lines) if lines else "        No configuration"

    def _generate_topology_doc(self) -> str:
        """Generate topology documentation string."""
        lines = []
        for edge in self.graph.edges:
            lines.append(f"        {edge.source} -> {edge.target}")
        return "\n".join(lines) if lines else "        No connections"

    def _generate_block_init(self) -> str:
        """
        Generate block initialization code.

        Returns:
            Block initialization statements
        """
        lines = []

        for node in self.graph.nodes:
            block_cap = self.registry.get_block(node.block_id)
            class_name = block_cap.name

            # Build parameter arguments
            params = []
            for param_name, param_value in node.params.items():
                # Substitute dimension values from configuration
                if isinstance(param_value, str) and param_value in self.config.bindings:
                    param_value = self.config.bindings[param_value]

                # Format parameter value
                if isinstance(param_value, str):
                    params.append(f"{param_name}='{param_value}'")
                else:
                    params.append(f"{param_name}={param_value}")

            # Add default parameters from capability spec
            for param_name, param_spec in block_cap.params.items():
                if param_name not in node.params and param_spec.default is not None:
                    if isinstance(param_spec.default, str):
                        params.append(f"{param_name}='{param_spec.default}'")
                    else:
                        params.append(f"{param_name}={param_spec.default}")

            param_str = ", ".join(params)
            lines.append(f"        self.{node.id} = {class_name}({param_str})")

        return "\n".join(lines)

    def _generate_forward_signature(self) -> Tuple[str, str]:
        """
        Generate forward method signature.

        Returns:
            Tuple of (args_string, args_doc_string)
        """
        # Determine inputs from graph
        if self.graph.inputs:
            # Explicit inputs specified
            args = ", ".join(self.graph.inputs)
            args_doc = "\n".join([
                f"            {inp}: Input tensor"
                for inp in self.graph.inputs
            ])
        else:
            # Default to single input 'x'
            args = "x"
            args_doc = "            x: Input tensor"

        return args, args_doc

    def _generate_return_signature(self) -> Tuple[str, str]:
        """
        Generate return type signature.

        Returns:
            Tuple of (return_type, return_doc)
        """
        if self.graph.outputs:
            if len(self.graph.outputs) == 1:
                return_type = "torch.Tensor"
                return_doc = f"            Output tensor from {self.graph.outputs[0]}"
            else:
                return_type = "Tuple[torch.Tensor, ...]"
                outputs_str = ", ".join(self.graph.outputs)
                return_doc = f"            Tuple of output tensors: ({outputs_str})"
        else:
            # Default to last node output
            return_type = "torch.Tensor"
            return_doc = "            Output tensor"

        return return_type, return_doc

    def _generate_shape_assertions(self) -> str:
        """
        Generate runtime shape assertions.

        Implements Req 8.5

        Returns:
            Shape assertion code
        """
        lines = []
        lines.append("        # Runtime shape assertions")

        # Add assertions for graph inputs
        if self.graph.inputs:
            for inp in self.graph.inputs:
                # Find the node that this input connects to
                input_edges = [e for e in self.graph.edges if e.source == inp]
                if input_edges:
                    target_node_id = input_edges[0].target
                    target_node = self._node_map[target_node_id]
                    block_cap = self.registry.get_block(target_node.block_id)

                    # Get input shape pattern
                    if input_edges[0].target_input in block_cap.inputs:
                        shape_pattern = block_cap.inputs[input_edges[0].target_input]
                        expected_dims = len(shape_pattern.pattern)

                        lines.append(
                            f"        assert {inp}.ndim == {expected_dims}, "
                            f"f'Expected {inp} to have {expected_dims} dimensions, got {{{inp}.ndim}}'"
                        )

        return "\n".join(lines) if len(lines) > 1 else "        pass  # No shape assertions"

    def _generate_forward_body(self) -> str:
        """
        Generate forward pass execution code.

        Implements Req 8.3, 8.4 (sequential and parallel)

        Returns:
            Forward pass code
        """
        if not self._execution_order:
            return "        pass"

        # Analyze graph structure
        structure = self._analyze_graph_structure()

        if structure['type'] == 'sequential':
            return self.generate_sequential(structure['nodes'])
        elif structure['type'] == 'parallel':
            return self.generate_parallel(structure['branches'], structure['merge_op'])
        else:
            # General case: execute in topological order
            return self._generate_general_forward()

    def _analyze_graph_structure(self) -> Dict[str, Any]:
        """
        Analyze graph structure to determine if it's sequential or parallel.

        Returns:
            Dict with 'type' and structure-specific keys
        """
        # Check if graph is purely sequential
        if self._is_sequential():
            return {
                'type': 'sequential',
                'nodes': [self._node_map[nid] for nid in self._execution_order]
            }

        # Check if graph has parallel branches
        branches = self._find_parallel_branches()
        if branches:
            return {
                'type': 'parallel',
                'branches': branches,
                'merge_op': 'add'  # Default merge operation
            }

        # General DAG
        return {'type': 'general'}

    def _is_sequential(self) -> bool:
        """Check if graph is a simple sequential chain."""
        if len(self.graph.nodes) == 0:
            return True

        # Each node should have at most one incoming and one outgoing edge
        in_degree = {node.id: 0 for node in self.graph.nodes}
        out_degree = {node.id: 0 for node in self.graph.nodes}

        for edge in self.graph.edges:
            if edge.source in out_degree:
                out_degree[edge.source] += 1
            if edge.target in in_degree:
                in_degree[edge.target] += 1

        # Sequential if all nodes have degree <= 1
        return all(deg <= 1 for deg in in_degree.values()) and \
               all(deg <= 1 for deg in out_degree.values())

    def _find_parallel_branches(self) -> Optional[List[List[GraphNode]]]:
        """
        Find parallel branches in graph.

        Returns:
            List of branches (each branch is a list of nodes), or None if no parallelism
        """
        # For now, return None to use general forward generation
        # Full parallel branch detection would require more complex analysis
        return None

    def generate_sequential(self, nodes: List[GraphNode]) -> str:
        """
        Generate code for sequential block chain.

        Implements Req 8.3

        Args:
            nodes: List of nodes in sequential order

        Returns:
            Generated forward pass code
        """
        lines = []

        # Determine input variable
        if self.graph.inputs:
            current_var = self.graph.inputs[0]
        else:
            current_var = "x"

        for i, node in enumerate(nodes):
            # Execute block
            next_var = f"{node.id}_out"
            lines.append(f"        {next_var} = self.{node.id}({current_var})")
            current_var = next_var

        return "\n".join(lines)

    def generate_parallel(
        self,
        branches: List[List[GraphNode]],
        merge_op: str = "add"
    ) -> str:
        """
        Generate code for parallel execution paths.

        Implements Req 8.4

        Args:
            branches: List of parallel branches (each is a list of nodes)
            merge_op: Merge operation ('add', 'concat', 'multiply')

        Returns:
            Generated forward pass code
        """
        lines = []

        # Execute each branch
        branch_results = []
        for i, branch in enumerate(branches):
            branch_var = f"branch_{i}"

            # Determine input for this branch
            if self.graph.inputs:
                current_var = self.graph.inputs[0]
            else:
                current_var = "x"

            # Execute branch nodes sequentially
            for node in branch:
                next_var = f"{node.id}_out"
                lines.append(f"        {next_var} = self.{node.id}({current_var})")
                current_var = next_var

            lines.append(f"        {branch_var} = {current_var}")
            branch_results.append(branch_var)

        # Merge results
        if merge_op == "add":
            merge_expr = " + ".join(branch_results)
        elif merge_op == "concat":
            merge_expr = f"torch.cat([{', '.join(branch_results)}], dim=-1)"
        elif merge_op == "multiply":
            merge_expr = " * ".join(branch_results)
        else:
            merge_expr = branch_results[0]  # Default to first branch

        lines.append(f"        merged = {merge_expr}")

        return "\n".join(lines)

    def _generate_general_forward(self) -> str:
        """
        Generate forward pass for general DAG structure.

        Returns:
            Forward pass code
        """
        lines = []

        # Track computed values
        computed: Set[str] = set()

        # Initialize graph inputs
        if self.graph.inputs:
            for inp in self.graph.inputs:
                computed.add(inp)
        else:
            computed.add("x")

        # Execute nodes in topological order
        for node_id in self._execution_order:
            node = self._node_map[node_id]

            # Find inputs for this node
            incoming_edges = [e for e in self.graph.edges if e.target == node_id]

            if not incoming_edges:
                # Node has no incoming edges
                if node_id in self.graph.inputs:
                    # This node is marked as an input, use it as input variable
                    input_var = node_id
                else:
                    # Use default input
                    input_var = "x" if "x" in computed else list(computed)[0] if computed else "x"
            else:
                # Use output from source node
                edge = incoming_edges[0]
                input_var = f"{edge.source}_out" if f"{edge.source}_out" in computed or edge.source not in computed else edge.source

            # Execute block
            output_var = f"{node_id}_out"
            lines.append(f"        {output_var} = self.{node_id}({input_var})")
            computed.add(output_var)

        return "\n".join(lines)

    def _generate_return_statement(self) -> str:
        """
        Generate return statement.

        Returns:
            Return statement code
        """
        if self.graph.outputs:
            if len(self.graph.outputs) == 1:
                output_id = self.graph.outputs[0]
                # Find the output variable
                return f"{output_id}_out"
            else:
                outputs = [f"{out}_out" for out in self.graph.outputs]
                return f"({', '.join(outputs)})"
        else:
            # Return output of last node
            if self._execution_order:
                last_node = self._execution_order[-1]
                return f"{last_node}_out"
            return "x"

    def _format_code(self, code: str) -> str:
        """
        Format generated code using black if available.

        Implements Req 8.8

        Args:
            code: Python code to format

        Returns:
            Formatted code (or original if black not available)
        """
        try:
            import black

            mode = black.FileMode()
            formatted = black.format_str(code, mode=mode)
            logger.debug("Code formatted with black")
            return formatted
        except ImportError:
            logger.debug("black not available, skipping code formatting")
            return code
        except Exception as e:
            logger.warning(f"Failed to format code with black: {e}")
            return code
