"""CompilationEngine for generating executable PyTorch code from validated graphs."""

import logging
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

from jinja2 import Template

from .block_registry import BlockRegistry
from .constraint_solver import Configuration
from .graph_loader import ArchitectureGraph, GraphEdge, GraphNode

logger = logging.getLogger(__name__)


class CompilationError(Exception):
    """Raised when code generation fails."""

    pass


# PyTorch module template using Jinja2
MODULE_TEMPLATE = """# Generated by NeuroScript v2 CompilationEngine
# Do not edit this file manually

import argparse
import json
import sys
import time
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Any, Dict, Optional, Tuple

{{ imports }}


class {{ class_name }}(nn.Module):
    \"\"\"
    {{ architecture_name }} - Generated neural architecture.

    Configuration:
{{ config_doc }}

    Graph topology:
{{ topology_doc }}
    \"\"\"

    def __init__(self, enable_monitoring: bool = True):
        super().__init__()
        self.enable_monitoring = enable_monitoring

        # Initialize blocks
{{ block_init }}

    def _emit_event(self, event_type: str, data: Dict[str, Any]) -> None:
        \"\"\"Emit monitoring event as JSON.\"\"\"
        if self.enable_monitoring:
            event = {
                'type': event_type,
                'timestamp': time.time(),
                'data': data
            }
            print(f"NEUROSCRIPT_EVENT:{json.dumps(event)}", flush=True)

    def _timing_context(self, block_name: str):
        \"\"\"Context manager for timing block execution.\"\"\"
        class TimingContext:
            def __init__(ctx_self, name, model):
                ctx_self.name = name
                ctx_self.model = model
                ctx_self.start_time = None
                ctx_self.input_shape = None
                ctx_self.output_shape = None

            def __enter__(ctx_self):
                ctx_self.start_time = time.time()
                return ctx_self

            def __exit__(ctx_self, *args):
                elapsed_ms = (time.time() - ctx_self.start_time) * 1000
                ctx_self.model._emit_event('block_forward', {
                    'block_name': ctx_self.name,
                    'input_shape': ctx_self.input_shape,
                    'output_shape': ctx_self.output_shape,
                    'time_ms': elapsed_ms
                })

        return TimingContext(block_name, self)

    def forward(self, {{ forward_args }}, enable_monitoring: bool = None) -> {{ forward_return }}:
        \"\"\"
        Forward pass through the architecture.

        Args:
{{ forward_args_doc }}
            enable_monitoring: Override monitoring for this forward pass

        Returns:
{{ forward_return_doc }}
        \"\"\"
        if enable_monitoring is not None:
            old_monitoring = self.enable_monitoring
            self.enable_monitoring = enable_monitoring

{{ shape_assertions }}

        # Execute computation graph with monitoring
{{ forward_body }}

        if enable_monitoring is not None:
            self.enable_monitoring = old_monitoring

        return {{ return_statement }}


def run_inference(model, input_shape, device='cpu'):
    \"\"\"Run inference mode with monitoring.\"\"\"
    model.eval()
    model.to(device)

    # Create sample input
    sample_input = torch.randn(*input_shape).to(device)

    # Emit execution start event
    model._emit_event('execution_start', {
        'mode': 'inference',
        'input_shape': list(input_shape),
        'device': str(device)
    })

    # Forward pass
    start_time = time.time()
    with torch.no_grad():
        output = model(sample_input)
    total_time = time.time() - start_time

    # Emit execution end event
    model._emit_event('execution_end', {
        'success': True,
        'total_time_seconds': total_time,
        'output_shape': list(output.shape)
    })

    print(f"\\nInference complete:")
    print(f"  Input shape: {sample_input.shape}")
    print(f"  Output shape: {output.shape}")
    print(f"  Total time: {total_time:.3f}s")


def run_training(model, input_shape, num_epochs=5, batch_size=32, device='cpu'):
    \"\"\"Run training mode with monitoring.\"\"\"
    model.train()
    model.to(device)

    # Create synthetic dataset (in real use, load actual data)
    num_samples = 1000
    X_train = torch.randn(num_samples, *input_shape[1:]).to(device)
    # Generate synthetic targets (classification task)
    # Adjust based on your model's output dimensions
    y_train = torch.randint(0, 10, (num_samples,)).to(device)

    # Setup optimizer and loss
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    # Calculate batches per epoch
    num_batches = num_samples // batch_size

    # Emit execution start event
    model._emit_event('execution_start', {
        'mode': 'training',
        'input_shape': list(input_shape),
        'num_epochs': num_epochs,
        'batch_size': batch_size,
        'device': str(device)
    })

    start_time = time.time()

    for epoch in range(1, num_epochs + 1):
        model._emit_event('epoch_start', {
            'epoch': epoch,
            'total_epochs': num_epochs
        })

        epoch_loss = 0.0
        correct = 0
        total = 0

        for batch_idx in range(num_batches):
            # Get batch
            start_idx = batch_idx * batch_size
            end_idx = start_idx + batch_size
            batch_X = X_train[start_idx:end_idx]
            batch_y = y_train[start_idx:end_idx]

            # Forward pass
            optimizer.zero_grad()
            outputs = model(batch_X, enable_monitoring=(batch_idx == 0))  # Only monitor first batch
            loss = criterion(outputs, batch_y)

            # Backward pass
            loss.backward()
            optimizer.step()

            # Track metrics
            epoch_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()

            # Emit batch complete event
            if batch_idx % 10 == 0:  # Report every 10 batches
                model._emit_event('batch_complete', {
                    'epoch': epoch,
                    'batch': batch_idx + 1,
                    'total_batches': num_batches,
                    'loss': loss.item(),
                    'metrics': {}
                })

        # Calculate epoch metrics
        avg_loss = epoch_loss / num_batches
        accuracy = correct / total

        # Emit epoch complete event
        model._emit_event('epoch_complete', {
            'epoch': epoch,
            'total_epochs': num_epochs,
            'train_loss': avg_loss,
            'metrics': {
                'accuracy': accuracy
            }
        })

        print(f"Epoch {epoch}/{num_epochs} - Loss: {avg_loss:.4f} - Acc: {accuracy:.4f}")

    total_time = time.time() - start_time

    # Emit execution end event
    model._emit_event('execution_end', {
        'success': True,
        'total_time_seconds': total_time
    })

    print(f"\\nTraining complete: {total_time:.2f}s")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Run {{ architecture_name }}')
    parser.add_argument('--mode', choices=['inference', 'training'], default='inference',
                        help='Execution mode (default: inference)')
    parser.add_argument('--epochs', type=int, default=5,
                        help='Number of training epochs (default: 5)')
    parser.add_argument('--batch-size', type=int, default=32,
                        help='Training batch size (default: 32)')
    parser.add_argument('--input-shape', type=str, default='1,512',
                        help='Input shape as comma-separated integers (default: 1,512)')
    parser.add_argument('--device', type=str, default='cpu',
                        help='Device to run on (default: cpu)')
    parser.add_argument('--no-monitoring', action='store_true',
                        help='Disable monitoring events')

    args = parser.parse_args()

    # Parse input shape
    input_shape = tuple(map(int, args.input_shape.split(',')))

    # Create model
    model = {{ class_name }}(enable_monitoring=not args.no_monitoring)

    # Run appropriate mode
    if args.mode == 'inference':
        run_inference(model, input_shape, device=args.device)
    else:
        run_training(model, input_shape, num_epochs=args.epochs,
                     batch_size=args.batch_size, device=args.device)
"""


class CompilationEngine:
    """
    Generates executable PyTorch code from validated architecture graphs.

    Implements Req 8.1, 8.2, 8.3, 8.4, 8.5
    """

    def __init__(
        self,
        graph: ArchitectureGraph,
        config: Configuration,
        registry: BlockRegistry
    ):
        """
        Initialize CompilationEngine.

        Args:
            graph: Validated architecture graph
            config: Concrete dimension configuration
            registry: Block registry for accessing block metadata
        """
        self.graph = graph
        self.config = config
        self.registry = registry
        self._node_map: Dict[str, GraphNode] = {node.id: node for node in graph.nodes}
        self._execution_order: Optional[List[str]] = None

    def compile(self, output_path: Path, class_name: str = "GeneratedModel") -> None:
        """
        Generate PyTorch module file.

        Implements Req 8.1, 8.2

        Args:
            output_path: Path to write generated Python module
            class_name: Name for generated nn.Module class

        Raises:
            CompilationError: If code generation fails
        """
        logger.info(f"Compiling architecture to {output_path}")

        try:
            # Compute execution order
            self._execution_order = self._compute_execution_order()

            # Generate all code components
            imports = self._generate_imports()
            config_doc = self._generate_config_doc()
            topology_doc = self._generate_topology_doc()
            block_init = self._generate_block_init()
            forward_args, forward_args_doc = self._generate_forward_signature()
            forward_return, forward_return_doc = self._generate_return_signature()
            shape_assertions = self._generate_shape_assertions()
            forward_body = self._generate_forward_body()
            return_statement = self._generate_return_statement()

            # Render template
            template = Template(MODULE_TEMPLATE)
            code = template.render(
                class_name=class_name,
                architecture_name=self.graph.metadata.get('name', 'UnnamedArchitecture'),
                imports=imports,
                config_doc=config_doc,
                topology_doc=topology_doc,
                block_init=block_init,
                forward_args=forward_args,
                forward_args_doc=forward_args_doc,
                forward_return=forward_return,
                forward_return_doc=forward_return_doc,
                shape_assertions=shape_assertions,
                forward_body=forward_body,
                return_statement=return_statement,
            )

            # Format code
            code = self._format_code(code)

            # Write to file
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            output_path.write_text(code)

            logger.info(f"Successfully compiled to {output_path}")

        except (OSError, IOError, ValueError, RuntimeError, KeyError, TypeError, AttributeError) as e:
            raise CompilationError(f"Failed to compile architecture: {e}") from e

    def _compute_execution_order(self) -> List[str]:
        """
        Compute topological execution order for nodes.

        Returns:
            List of node IDs in execution order

        Raises:
            CompilationError: If graph has cycles (should not happen after validation)
        """
        # Build adjacency list and in-degree count
        adj: Dict[str, List[str]] = {node.id: [] for node in self.graph.nodes}
        in_degree: Dict[str, int] = {node.id: 0 for node in self.graph.nodes}

        for edge in self.graph.edges:
            if edge.source in adj and edge.target in adj:
                adj[edge.source].append(edge.target)
                in_degree[edge.target] += 1

        # Kahn's algorithm for topological sort
        queue = [node_id for node_id, degree in in_degree.items() if degree == 0]
        order = []

        while queue:
            # Sort for deterministic ordering
            queue.sort()
            node_id = queue.pop(0)
            order.append(node_id)

            for neighbor in adj[node_id]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)

        if len(order) != len(self.graph.nodes):
            raise CompilationError("Graph contains cycles (validation should have caught this)")

        return order

    def _generate_imports(self) -> str:
        """
        Generate import statements for block implementations.

        Implements Req 8.2

        Returns:
            Import statements as string
        """
        imports = set()

        for node in self.graph.nodes:
            block_path = self.registry.get_block_path(node.block_id)
            # Convert block path to Python import
            # Example: blocks/linear -> from blocks.linear.module import Linear
            block_name = node.block_id

            # Get the block capability to find the actual class name
            block_cap = self.registry.get_block(node.block_id)
            class_name = block_cap.name

            import_stmt = f"from blocks.{block_name}.module import {class_name}"
            imports.add(import_stmt)

        return "\n".join(sorted(imports))

    def _generate_config_doc(self) -> str:
        """Generate configuration documentation string."""
        lines = []
        for dim, value in sorted(self.config.bindings.items()):
            lines.append(f"        {dim} = {value}")

        if self.config.estimated_params > 0:
            lines.append(f"        Total parameters: {self.config.estimated_params:,}")
        if self.config.estimated_memory_gb > 0:
            lines.append(f"        Estimated memory: {self.config.estimated_memory_gb:.2f} GB")

        return "\n".join(lines) if lines else "        No configuration"

    def _generate_topology_doc(self) -> str:
        """Generate topology documentation string."""
        lines = []
        for edge in self.graph.edges:
            lines.append(f"        {edge.source} -> {edge.target}")
        return "\n".join(lines) if lines else "        No connections"

    def _generate_block_init(self) -> str:
        """
        Generate block initialization code.

        Returns:
            Block initialization statements
        """
        lines = []

        for node in self.graph.nodes:
            block_cap = self.registry.get_block(node.block_id)
            class_name = block_cap.name

            # Build parameter arguments
            params = []
            for param_name, param_value in node.params.items():
                # Substitute dimension values from configuration
                if isinstance(param_value, str) and param_value in self.config.bindings:
                    param_value = self.config.bindings[param_value]

                # Format parameter value
                if isinstance(param_value, str):
                    params.append(f"{param_name}='{param_value}'")
                else:
                    params.append(f"{param_name}={param_value}")

            # Add default parameters from capability spec
            for param_name, param_spec in block_cap.params.items():
                if param_name not in node.params and param_spec.default is not None:
                    if isinstance(param_spec.default, str):
                        params.append(f"{param_name}='{param_spec.default}'")
                    else:
                        params.append(f"{param_name}={param_spec.default}")

            param_str = ", ".join(params)
            lines.append(f"        self.{node.id} = {class_name}({param_str})")

        return "\n".join(lines)

    def _generate_forward_signature(self) -> Tuple[str, str]:
        """
        Generate forward method signature.

        Returns:
            Tuple of (args_string, args_doc_string)
        """
        # Determine inputs from graph
        if self.graph.inputs:
            # Explicit inputs specified
            args = ", ".join(self.graph.inputs)
            args_doc = "\n".join([
                f"            {inp}: Input tensor"
                for inp in self.graph.inputs
            ])
        else:
            # Default to single input 'x'
            args = "x"
            args_doc = "            x: Input tensor"

        return args, args_doc

    def _generate_return_signature(self) -> Tuple[str, str]:
        """
        Generate return type signature.

        Returns:
            Tuple of (return_type, return_doc)
        """
        if self.graph.outputs:
            if len(self.graph.outputs) == 1:
                return_type = "torch.Tensor"
                return_doc = f"            Output tensor from {self.graph.outputs[0]}"
            else:
                return_type = "Tuple[torch.Tensor, ...]"
                outputs_str = ", ".join(self.graph.outputs)
                return_doc = f"            Tuple of output tensors: ({outputs_str})"
        else:
            # Default to last node output
            return_type = "torch.Tensor"
            return_doc = "            Output tensor"

        return return_type, return_doc

    def _generate_shape_assertions(self) -> str:
        """
        Generate runtime shape assertions.

        Implements Req 8.5

        Returns:
            Shape assertion code
        """
        lines = []
        lines.append("        # Runtime shape assertions")

        # Add assertions for graph inputs
        if self.graph.inputs:
            for inp in self.graph.inputs:
                # Find the node that this input connects to
                input_edges = [e for e in self.graph.edges if e.source == inp]
                if input_edges:
                    target_node_id = input_edges[0].target
                    target_node = self._node_map[target_node_id]
                    block_cap = self.registry.get_block(target_node.block_id)

                    # Get input shape pattern
                    if input_edges[0].target_input in block_cap.inputs:
                        shape_pattern = block_cap.inputs[input_edges[0].target_input]
                        expected_dims = len(shape_pattern.pattern)

                        lines.append(
                            f"        assert {inp}.ndim == {expected_dims}, "
                            f"f'Expected {inp} to have {expected_dims} dimensions, got {{{inp}.ndim}}'"
                        )

        return "\n".join(lines) if len(lines) > 1 else "        pass  # No shape assertions"

    def _generate_forward_body(self) -> str:
        """
        Generate forward pass execution code.

        Implements Req 8.3, 8.4 (sequential and parallel)

        Returns:
            Forward pass code
        """
        if not self._execution_order:
            return "        pass"

        # Analyze graph structure
        structure = self._analyze_graph_structure()

        if structure['type'] == 'sequential':
            return self.generate_sequential(structure['nodes'])
        elif structure['type'] == 'parallel':
            return self.generate_parallel(structure['branches'], structure['merge_op'])
        else:
            # General case: execute in topological order
            return self._generate_general_forward()

    def _analyze_graph_structure(self) -> Dict[str, Any]:
        """
        Analyze graph structure to determine if it's sequential or parallel.

        Returns:
            Dict with 'type' and structure-specific keys
        """
        # Check if graph is purely sequential
        if self._is_sequential():
            return {
                'type': 'sequential',
                'nodes': [self._node_map[nid] for nid in self._execution_order] if self._execution_order is not None else list()
            }

        # Check if graph has parallel branches
        branches = self._find_parallel_branches()
        if branches:
            return {
                'type': 'parallel',
                'branches': branches,
                'merge_op': 'add'  # Default merge operation
            }

        # General DAG
        return {'type': 'general'}

    def _is_sequential(self) -> bool:
        """Check if graph is a simple sequential chain."""
        if len(self.graph.nodes) == 0:
            return True

        # Each node should have at most one incoming and one outgoing edge
        in_degree = {node.id: 0 for node in self.graph.nodes}
        out_degree = {node.id: 0 for node in self.graph.nodes}

        for edge in self.graph.edges:
            if edge.source in out_degree:
                out_degree[edge.source] += 1
            if edge.target in in_degree:
                in_degree[edge.target] += 1

        # Sequential if all nodes have degree <= 1
        return all(deg <= 1 for deg in in_degree.values()) and \
               all(deg <= 1 for deg in out_degree.values())

    def _find_parallel_branches(self) -> Optional[List[List[GraphNode]]]:
        """
        Find parallel branches in graph.

        Returns:
            List of branches (each branch is a list of nodes), or None if no parallelism
        """
        # For now, return None to use general forward generation
        # Full parallel branch detection would require more complex analysis
        return None

    def generate_sequential(self, nodes: List[GraphNode]) -> str:
        """
        Generate code for sequential block chain.

        Implements Req 8.3

        Args:
            nodes: List of nodes in sequential order

        Returns:
            Generated forward pass code
        """
        lines = []

        # Determine input variable
        if self.graph.inputs:
            current_var = self.graph.inputs[0]
        else:
            current_var = "x"

        for i, node in enumerate(nodes):
            # Execute block with monitoring
            next_var = f"{node.id}_out"
            lines.append(f"        with self._timing_context('{node.id}') as ctx:")
            lines.append(f"            ctx.input_shape = list({current_var}.shape)")
            lines.append(f"            {next_var} = self.{node.id}({current_var})")
            lines.append(f"            ctx.output_shape = list({next_var}.shape)")
            current_var = next_var

        return "\n".join(lines)

    def generate_parallel(
        self,
        branches: List[List[GraphNode]],
        merge_op: str = "add"
    ) -> str:
        """
        Generate code for parallel execution paths.

        Implements Req 8.4

        Args:
            branches: List of parallel branches (each is a list of nodes)
            merge_op: Merge operation ('add', 'concat', 'multiply')

        Returns:
            Generated forward pass code
        """
        lines = []

        # Execute each branch
        branch_results = []
        for i, branch in enumerate(branches):
            branch_var = f"branch_{i}"

            # Determine input for this branch
            if self.graph.inputs:
                current_var = self.graph.inputs[0]
            else:
                current_var = "x"

            # Execute branch nodes sequentially
            for node in branch:
                next_var = f"{node.id}_out"
                lines.append(f"        {next_var} = self.{node.id}({current_var})")
                current_var = next_var

            lines.append(f"        {branch_var} = {current_var}")
            branch_results.append(branch_var)

        # Merge results
        if merge_op == "add":
            merge_expr = " + ".join(branch_results)
        elif merge_op == "concat":
            merge_expr = f"torch.cat([{', '.join(branch_results)}], dim=-1)"
        elif merge_op == "multiply":
            merge_expr = " * ".join(branch_results)
        else:
            merge_expr = branch_results[0]  # Default to first branch

        lines.append(f"        merged = {merge_expr}")

        return "\n".join(lines)

    def _generate_general_forward(self) -> str:
        """
        Generate forward pass for general DAG structure.

        Returns:
            Forward pass code
        """
        lines = []

        # Track computed values
        computed: Set[str] = set()

        # Initialize graph inputs
        if self.graph.inputs:
            for inp in self.graph.inputs:
                computed.add(inp)
        else:
            computed.add("x")

        # Execute nodes in topological order
        if (self._execution_order is not None):
            for node_id in self._execution_order:
                node = self._node_map[node_id]

                # Find inputs for this node
                incoming_edges = [e for e in self.graph.edges if e.target == node_id]

                if not incoming_edges:
                    # Node has no incoming edges
                    if node_id in self.graph.inputs:
                        # This node is marked as an input, use it as input variable
                        input_var = node_id
                    else:
                        # Use default input
                        input_var = "x" if "x" in computed else list(computed)[0] if computed else "x"
                else:
                    # Use output from source node
                    edge = incoming_edges[0]
                    input_var = f"{edge.source}_out" if f"{edge.source}_out" in computed or edge.source not in computed else edge.source

                # Execute block with monitoring
                output_var = f"{node_id}_out"
                lines.append(f"        with self._timing_context('{node_id}') as ctx:")
                lines.append(f"            ctx.input_shape = list({input_var}.shape)")
                lines.append(f"            {output_var} = self.{node_id}({input_var})")
                lines.append(f"            ctx.output_shape = list({output_var}.shape)")
                computed.add(output_var)

        return "\n".join(lines)

    def _generate_return_statement(self) -> str:
        """
        Generate return statement.

        Returns:
            Return statement code
        """
        if self.graph.outputs:
            if len(self.graph.outputs) == 1:
                output_id = self.graph.outputs[0]
                # Find the output variable
                return f"{output_id}_out"
            else:
                outputs = [f"{out}_out" for out in self.graph.outputs]
                return f"({', '.join(outputs)})"
        else:
            # Return output of last node
            if self._execution_order:
                last_node = self._execution_order[-1]
                return f"{last_node}_out"
            return "x"

    def _format_code(self, code: str) -> str:
        """
        Format generated code using black if available.

        Implements Req 8.8

        Args:
            code: Python code to format

        Returns:
            Formatted code (or original if black not available)
        """
        try:
            import black

            mode = black.FileMode()
            formatted = black.format_str(code, mode=mode)
            logger.debug("Code formatted with black")
            return formatted
        except ImportError:
            logger.debug("black not available, skipping code formatting")
            return code
        except (ValueError, RuntimeError) as e:
            logger.warning(f"Failed to format code with black: {e}")
            return code
