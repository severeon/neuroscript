# Generated by NeuroScript v2 CompilationEngine
# Do not edit this file manually

import argparse
import json
import sys
import time
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Any, Dict, Optional, Tuple

from blocks.linear.module import Linear


class GeneratedModel(nn.Module):
    """
    UnnamedArchitecture - Generated neural architecture.

    Configuration:
        No configuration

    Graph topology:
        layer1 -> layer2
        layer2 -> layer3
        layer3 -> layer4
        layer4 -> layer5
    """

    def __init__(self, enable_monitoring: bool = True):
        super().__init__()
        self.enable_monitoring = enable_monitoring

        # Initialize blocks
        self.layer1 = Linear(in_features=512, out_features=256, bias=True)
        self.layer2 = Linear(in_features=512, out_features=256, bias=True)
        self.layer3 = Linear(in_features=512, out_features=256, bias=True)
        self.layer4 = Linear(in_features=512, out_features=256, bias=True)
        self.layer5 = Linear(in_features=256, out_features=128, bias=True)

    def _emit_event(self, event_type: str, data: Dict[str, Any]) -> None:
        """Emit monitoring event as JSON."""
        if self.enable_monitoring:
            event = {"type": event_type, "timestamp": time.time(), "data": data}
            print(f"NEUROSCRIPT_EVENT:{json.dumps(event)}", flush=True)

    def _timing_context(self, block_name: str):
        """Context manager for timing block execution."""

        class TimingContext:
            def __init__(ctx_self, name, model):
                ctx_self.name = name
                ctx_self.model = model
                ctx_self.start_time = None
                ctx_self.input_shape = None
                ctx_self.output_shape = None

            def __enter__(ctx_self):
                ctx_self.start_time = time.time()
                return ctx_self

            def __exit__(ctx_self, *args):
                elapsed_ms = (time.time() - ctx_self.start_time) * 1000
                ctx_self.model._emit_event(
                    "block_forward",
                    {
                        "block_name": ctx_self.name,
                        "input_shape": ctx_self.input_shape,
                        "output_shape": ctx_self.output_shape,
                        "time_ms": elapsed_ms,
                    },
                )

        return TimingContext(block_name, self)

    def forward(self, layer1, enable_monitoring: bool = None) -> torch.Tensor:
        """
        Forward pass through the architecture.

        Args:
            layer1: Input tensor
            enable_monitoring: Override monitoring for this forward pass

        Returns:
            Output tensor from layer5
        """
        if enable_monitoring is not None:
            old_monitoring = self.enable_monitoring
            self.enable_monitoring = enable_monitoring

        # Runtime shape assertions
        assert (
            layer1.ndim == 2
        ), f"Expected layer1 to have 2 dimensions, got {layer1.ndim}"

        # Execute computation graph with monitoring
        with self._timing_context("layer1") as ctx:
            ctx.input_shape = list(layer1.shape)
            layer1_out = self.layer1(layer1)
            ctx.output_shape = list(layer1_out.shape)
        with self._timing_context("layer2") as ctx:
            ctx.input_shape = list(layer1_out.shape)
            layer2_out = self.layer2(layer1_out)
            ctx.output_shape = list(layer2_out.shape)
        with self._timing_context("layer3") as ctx:
            ctx.input_shape = list(layer2_out.shape)
            layer3_out = self.layer3(layer2_out)
            ctx.output_shape = list(layer3_out.shape)
        with self._timing_context("layer4") as ctx:
            ctx.input_shape = list(layer3_out.shape)
            layer4_out = self.layer4(layer3_out)
            ctx.output_shape = list(layer4_out.shape)
        with self._timing_context("layer5") as ctx:
            ctx.input_shape = list(layer4_out.shape)
            layer5_out = self.layer5(layer4_out)
            ctx.output_shape = list(layer5_out.shape)

        if enable_monitoring is not None:
            self.enable_monitoring = old_monitoring

        return layer5_out


def run_inference(model, input_shape, device="cpu"):
    """Run inference mode with monitoring."""
    model.eval()
    model.to(device)

    # Create sample input
    sample_input = torch.randn(*input_shape).to(device)

    # Emit execution start event
    model._emit_event(
        "execution_start",
        {"mode": "inference", "input_shape": list(input_shape), "device": str(device)},
    )

    # Forward pass
    start_time = time.time()
    with torch.no_grad():
        output = model(sample_input)
    total_time = time.time() - start_time

    # Emit execution end event
    model._emit_event(
        "execution_end",
        {
            "success": True,
            "total_time_seconds": total_time,
            "output_shape": list(output.shape),
        },
    )

    print(f"\nInference complete:")
    print(f"  Input shape: {sample_input.shape}")
    print(f"  Output shape: {output.shape}")
    print(f"  Total time: {total_time:.3f}s")


def run_training(model, input_shape, num_epochs=5, batch_size=32, device="cpu"):
    """Run training mode with monitoring."""
    model.train()
    model.to(device)

    # Create synthetic dataset (in real use, load actual data)
    num_samples = 1000
    X_train = torch.randn(num_samples, *input_shape[1:]).to(device)
    # Generate synthetic targets (classification task)
    # Adjust based on your model's output dimensions
    y_train = torch.randint(0, 10, (num_samples,)).to(device)

    # Setup optimizer and loss
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    # Calculate batches per epoch
    num_batches = num_samples // batch_size

    # Emit execution start event
    model._emit_event(
        "execution_start",
        {
            "mode": "training",
            "input_shape": list(input_shape),
            "num_epochs": num_epochs,
            "batch_size": batch_size,
            "device": str(device),
        },
    )

    start_time = time.time()

    for epoch in range(1, num_epochs + 1):
        model._emit_event("epoch_start", {"epoch": epoch, "total_epochs": num_epochs})

        epoch_loss = 0.0
        correct = 0
        total = 0

        for batch_idx in range(num_batches):
            # Get batch
            start_idx = batch_idx * batch_size
            end_idx = start_idx + batch_size
            batch_X = X_train[start_idx:end_idx]
            batch_y = y_train[start_idx:end_idx]

            # Forward pass
            optimizer.zero_grad()
            outputs = model(
                batch_X, enable_monitoring=(batch_idx == 0)
            )  # Only monitor first batch
            loss = criterion(outputs, batch_y)

            # Backward pass
            loss.backward()
            optimizer.step()

            # Track metrics
            epoch_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()

            # Emit batch complete event
            if batch_idx % 10 == 0:  # Report every 10 batches
                model._emit_event(
                    "batch_complete",
                    {
                        "epoch": epoch,
                        "batch": batch_idx + 1,
                        "total_batches": num_batches,
                        "loss": loss.item(),
                        "metrics": {},
                    },
                )

        # Calculate epoch metrics
        avg_loss = epoch_loss / num_batches
        accuracy = correct / total

        # Emit epoch complete event
        model._emit_event(
            "epoch_complete",
            {
                "epoch": epoch,
                "total_epochs": num_epochs,
                "train_loss": avg_loss,
                "metrics": {"accuracy": accuracy},
            },
        )

        print(
            f"Epoch {epoch}/{num_epochs} - Loss: {avg_loss:.4f} - Acc: {accuracy:.4f}"
        )

    total_time = time.time() - start_time

    # Emit execution end event
    model._emit_event(
        "execution_end", {"success": True, "total_time_seconds": total_time}
    )

    print(f"\nTraining complete: {total_time:.2f}s")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run UnnamedArchitecture")
    parser.add_argument(
        "--mode",
        choices=["inference", "training"],
        default="inference",
        help="Execution mode (default: inference)",
    )
    parser.add_argument(
        "--epochs", type=int, default=5, help="Number of training epochs (default: 5)"
    )
    parser.add_argument(
        "--batch-size", type=int, default=32, help="Training batch size (default: 32)"
    )
    parser.add_argument(
        "--input-shape",
        type=str,
        default="1,512",
        help="Input shape as comma-separated integers (default: 1,512)",
    )
    parser.add_argument(
        "--device", type=str, default="cpu", help="Device to run on (default: cpu)"
    )
    parser.add_argument(
        "--no-monitoring", action="store_true", help="Disable monitoring events"
    )

    args = parser.parse_args()

    # Parse input shape
    input_shape = tuple(map(int, args.input_shape.split(",")))

    # Create model
    model = GeneratedModel(enable_monitoring=not args.no_monitoring)

    # Run appropriate mode
    if args.mode == "inference":
        run_inference(model, input_shape, device=args.device)
    else:
        run_training(
            model,
            input_shape,
            num_epochs=args.epochs,
            batch_size=args.batch_size,
            device=args.device,
        )
